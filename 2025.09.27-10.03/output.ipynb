{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e2a1e-f544-48d2-8a8c-8b0c45fd196f",
   "metadata": {},
   "source": [
    "# 9.27-10.3 周报\n",
    "\n",
    "> Author: 彭日骏\n",
    "> Edit Time: 2025/9/27-2025/10/3\n",
    "\n",
    "---\n",
    "## Plan\n",
    "- [ ] 读懂Attention Is All You Need\n",
    "---\n",
    "## *Attention Is All You Need* 论文阅读\n",
    "### *单词扫盲*\n",
    "    dominant 主流的\n",
    "    sequence transduction models 序列转换模型\n",
    "    sequential 顺序的\n",
    "    convolutional 卷积的\n",
    "    mechanism 机制\n",
    "    architecture 架构\n",
    "    recurrence 循环\n",
    "    parallel 并行\n",
    "    fraction 部分\n",
    "    evaluate 评估\n",
    "    *self-attention 自注意力\n",
    "    implement 实施\n",
    "    parameter 参数；范围\n",
    "    tun 调试\n",
    "    model variant 模型变体\n",
    "    codebase 代码库\n",
    "    inference 推理\n",
    "    visualization 可视化\n",
    "    state of the art approaches 最先进的方法\n",
    "    align 对齐\n",
    "    constraint 限制\n",
    "    conjunction 结合\n",
    "    eschew 避开\n",
    "    constant 恒定的\n",
    "    albeit 尽管\n",
    "    counteract 抵消\n",
    "### *Abstract* 关键词\n",
    "        1. attention mechanism（注意力机制）\n",
    "        2. scaled dot-product attention\n",
    "        3. multi-head attention\n",
    "        4. parameter-free position representation\n",
    "### *Introduction* 关键词\n",
    "        1. 旧时处理transduction problem: Recurrent neutral networks 循环神经网络\n",
    "            --- long short-term memory 长短期记忆网络（LSTM）[13]\n",
    "                [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,9(8):1735–1780, 1997.\n",
    "            --- gated recurrent neutral networks 门控循环神经网络（GRU）[7]\n",
    "                [7]\n",
    "            --- conditional computation 条件计算作优化计算效率和模型性能都有提高\n",
    "            --- fundamental constraint of sequential computation 仍旧存在\n",
    "            --- usually be used in conjunction with attention mechanisms\n",
    "            --- 我结合了知乎 https://zhuanlan.zhihu.com/p/123211148 与论文内容进行了解\n",
    "            --- 得到解释 什么是\n",
    "        2. Transformer 模型\n",
    "            --- 完全依赖一个 attention mechanism（注意力机制） 排除掉recurrence 对输入输出的全局依赖关系进行建模的一个模型\n",
    "            --- 并行化更好、效率更高、且translation的质量也更高了\n",
    "### *Background* 关键词\n",
    "        1. Extended Neural GPU, ByteNet, ConvS2S\n",
    "            --- 都使用 CNN(convolutional neural networks) as basic block, 并行计算所有输入输出的隐藏表示的位置？？\n",
    "            --- 是为了更好解决transduction problem而进行的过往的模型改进\n",
    "            --- 减少sequential computation本来也是这些改进算法的目标\n",
    "            --- 共性问题：将任意的两个输入输出信号的位置关联起来的并行计算的操作数，会随其位置之间的距离而快速增长（在ConvS2S模型中会线性增长，在ByteNet中会对数增长），使得模型更难学习两个较远信号之间的依赖关系\n",
    "            --- 相较于上述模型，Transformer采用average attention-weighted positions(?), 使得操作数减少到一个恒定数量，但 effective resolution（有效分辨率）降低了\n",
    "            --- Transformer又通过 Multi-Head Attention（多头注意力） 抵消这种影响\n",
    "            --- 结合了知乎文章 https://zhuanlan.zhihu.com/p/635438713 了解了CNN\n",
    "            --- \n",
    "        2. Self-attention（自注意力）\n",
    "            --- 又称 intra-attention（内部注意力）\n",
    "            --- 将单个序列中不同位置关联起来以计算序列表示的注意力机制\n",
    "            --- 已具有广泛的应用场景：reading comprehension, abstractive summarization,  textual entailment, learning task-independent sentence representations(??)\n",
    "        3. End-to-end memory networks（端到端记忆网络）\n",
    "            --- based on  recurrent attention machanism（循环注意力机制），而非 sequence-aligned recurrence（序列对齐循环）\n",
    "            --- 应用于简单的语言问题回答\n",
    "        4. Transformer\n",
    "            --- 完全依赖 self-attention 计算输入输出表示而不需要使用 sequence-aligned RNNs（序列对齐的循环神经网络）或卷积\n",
    "            --- 接下来详细介绍 Transformer 相较于其他的模型的优势在哪里\n",
    "\n",
    "不出意料的一进入Model Architecture Part我就看不懂了，然后我决定先去学习在Abstract, Introduction, Background中我所不理解的那些关键词，从神经网络开始进行温故和学习，从中我在模型的建立和推导下了一些苦功夫理解，以前对概念只是模糊的理解，力求通过学习后我能够对概念获得更深刻的了解，并了解模型和模型间的区别，应用场景的不同，和各种分门别类的优化。\n",
    "\n",
    "然后我就通过B站和知乎寻找资源学习了以下的内容\n",
    "### *多层感知机（MLP）*\n",
    "![mlp-1](.\\\\note\\\\mlp-1.jpg \"对MLP模型的基础概念理解\")\n",
    "![mlp-2](.\\\\note\\\\mlp-2.jpg \"对MLP模型反向传播的算法推导\")\n",
    "\n",
    "### *卷积神经网络（CNN）*\n",
    "![cnn-1](.\\\\note\\\\cnn-1.jpg \"对CNN模型的基础概念理解\")\n",
    "对CNN模型反向传播的推导正在努力啃数学公式ing...\n",
    "\n",
    "### *循环神经网络（RNN）*\n",
    "![rnn-1](.\\\\note\\\\rnn-1.jpg \"对RNN模型的基础概念理解\")\n",
    "对RNN模型反向传播的推导正在努力啃数学公式ing...\n",
    "\n",
    "### *长短期记忆网络（LSTM）*\n",
    "![lstm-1](.\\\\note\\\\lstm-1.jpg \"对LSTM模型的基础概念理解\")\n",
    "对LSTM模型反向传播的推导正在努力啃数学公式ing...\n",
    "\n",
    "---\n",
    "我本来是想要速通一下这些模型的基础概念，跳过了其中反向传播中复杂的数学推导，然后迅速转战attention和Transformer，这样可以达成我的周报Plan。但是很遗憾我尽管对这些模型有了基础概念的理解，但我最终并没有看懂attention mechanism也就是所谓的注意力机制，更加谈不了自注意力、多头注意力等机制去理解Transformer了。\n",
    "我找到了一篇这个 https://www.zybuluo.com/hanbingtao/note/485480 其中每个模型进行反向传播的数学推导相当优美，并且有相应的代码实践\n",
    "\n",
    "---\n",
    "## 本周总结\n",
    "\n",
    "本周Plan并没有达到预期，以下是我的反思：\n",
    "\n",
    "首先是因为确实“万事开头难”，我一开始确实不知所措，蒙头转向，我还把Attention Is All You Need的论文打印了下来，直接对着英文翻看，效果并不好，反而像是在做英语阅读。\n",
    "\n",
    "我后来慢慢找到了一个方法：大概就是通过一些通俗易懂的讲解快速入门对一个模型，比如CNN，的一个全局观，知道他的工作原理都有那几步，设置这些步骤分别是为了实现些什么，然后再去找更专业的文章去了解（如上面【注】中的文章），去详细弄懂其中的工作原理和机制，最终再回到英语论文中，和国际接轨，了解原始文献中到底是怎么表述的，论文中是怎么一步步讲解他/她提出的模型的。这样的方法可以让我由通俗理解到深刻理解，最后回到原始论文（毕竟论文本身学术性很高，语言高度精炼浓缩）的时候能更快理解和明白论文的意思。\n",
    "\n",
    "---\n",
    "根据我2025/9/27-2025/10/3的学习情况，我下周的目标订立如下：\n",
    "- [ ] 完成CNN, RNN, LSTM, GRU, Transformer的基础概念学习以及算法推导\n",
    "- [ ] 读懂Attention Is All You Need\n",
    "\n",
    "经过上周的摸打滚爬，我有信心在新的一周完成更多知识的学习，请老师批评和指正！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
