# 9.27-10.3 周报
> Author: 彭日骏
> Edit Time: 2025/9/27-2025/10/3

---
## Plan
- [ ] 读懂Attention Is All You Need

---
## *Attention Is All You Need* 论文阅读
### *单词扫盲*
    dominant 主流的
    sequence transduction models 序列转换模型
    sequential 顺序的
    convolutional 卷积的
    mechanism 机制
    architecture 架构
    recurrence 循环
    parallel 并行
    fraction 部分
    evaluate 评估
    *self-attention 自注意力
    implement 实施
    parameter 参数；范围
    tun 调试
    model variant 模型变体
    codebase 代码库
    inference 推理
    visualization 可视化
    state of the art approaches 最先进的方法
    align 对齐
    constraint 限制
    conjunction 结合
    eschew 避开
    constant 恒定的
    albeit 尽管
    counteract 抵消
### *Abstract* 关键词
        1. attention mechanism（注意力机制）
        2. scaled dot-product attention
        3. multi-head attention
        4. parameter-free position representation
### *Introduction* 关键词
        1. 旧时处理transduction problem: Recurrent neutral networks 循环神经网络
            --- long short-term memory 长短期记忆网络（LSTM）[13]
                [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,9(8):1735–1780, 1997.
            --- gated recurrent neutral networks 门控循环神经网络（GRU）[7]
                [7]
            --- conditional computation 条件计算作优化计算效率和模型性能都有提高
            --- fundamental constraint of sequential computation 仍旧存在
            --- usually be used in conjunction with attention mechanisms
            --- 我结合了知乎 https://zhuanlan.zhihu.com/p/123211148 与论文内容进行了解
            --- 得到解释 什么是
        2. Transformer 模型
            --- 完全依赖一个 attention mechanism（注意力机制） 排除掉recurrence 对输入输出的全局依赖关系进行建模的一个模型
            --- 并行化更好、效率更高、且translation的质量也更高了
### *Background* 关键词
        1. Extended Neural GPU, ByteNet, ConvS2S
            --- 都使用 CNN(convolutional neural networks) as basic block, 并行计算所有输入输出的隐藏表示的位置？？
            --- 是为了更好解决transduction problem而进行的过往的模型改进
            --- 减少sequential computation本来也是这些改进算法的目标
            --- 共性问题：将任意的两个输入输出信号的位置关联起来的并行计算的操作数，会随其位置之间的距离而快速增长（在ConvS2S模型中会线性增长，在ByteNet中会对数增长），使得模型更难学习两个较远信号之间的依赖关系
            --- 相较于上述模型，Transformer采用average attention-weighted positions(?), 使得操作数减少到一个恒定数量，但 effective resolution（有效分辨率）降低了
            --- Transformer又通过 Multi-Head Attention（多头注意力） 抵消这种影响
            --- 结合了知乎文章 https://zhuanlan.zhihu.com/p/635438713 了解了CNN
            --- 
        2. Self-attention（自注意力）
            --- 又称 intra-attention（内部注意力）
            --- 将单个序列中不同位置关联起来以计算序列表示的注意力机制
            --- 已具有广泛的应用场景：reading comprehension, abstractive summarization,  textual entailment, learning task-independent sentence representations(??)
        3. End-to-end memory networks（端到端记忆网络）
            --- based on  recurrent attention machanism（循环注意力机制），而非 sequence-aligned recurrence（序列对齐循环）
            --- 应用于简单的语言问题回答
        4. Transformer
            --- 完全依赖 self-attention 计算输入输出表示而不需要使用 sequence-aligned RNNs（序列对齐的循环神经网络）或卷积
            --- 接下来详细介绍 Transformer 相较于其他的模型的优势在哪里

不出意料的一进入Model Architecture Part我就看不懂了，然后我决定先去学习在Abstract, Introduction, Background中我所不理解的那些关键词，从神经网络开始进行温故和学习，从中我在模型的建立和推导下了一些苦功夫理解，以前对概念只是模糊的理解，力求通过学习后我能够对概念获得更深刻的了解，并了解模型和模型间的区别，应用场景的不同，和各种分门别类的优化。

然后我就通过B站和知乎寻找资源学习了以下的内容
### *多层感知机（MLP）*
[mlp-1](.\\note\\mlp-1.jpg "对MLP模型的基础概念理解")
[mlp-2](.\\note\\mlp-2.jpg "对MLP模型反向传播的算法推导")

### *卷积神经网络（CNN）*
[cnn-1](.\\note\\cnn-1.jpg "对CNN模型的基础概念理解")
对CNN模型反向传播的推导正在努力啃数学公式ing...

### *循环神经网络（RNN）*
[rnn-1](.\\note\\rnn-1.jpg "对RNN模型的基础概念理解")
对RNN模型反向传播的推导正在努力啃数学公式ing...

### *长短期记忆网络（LSTM）*
[lstm-1](.\\note\\lstm-1.jpg "对LSTM模型的基础概念理解")
对LSTM模型反向传播的推导正在努力啃数学公式ing...

---
我本来是想要速通一下这些模型的基础概念，跳过了其中反向传播中复杂的数学推导，然后迅速转战attention和Transformer，这样可以达成我的周报Plan。但是很遗憾我尽管对这些模型有了基础概念的理解，但我最终并没有看懂attention mechanism也就是所谓的注意力机制，更加谈不了自注意力、多头注意力等机制去理解Transformer了。
我找到了一篇这个 https://www.zybuluo.com/hanbingtao/note/485480 其中每个模型进行反向传播的数学推导相当优美，并且有相应的代码实践

---
## 本周总结

本周Plan并没有达到预期，以下是我的反思：

首先是因为确实“万事开头难”，我一开始确实不知所措，蒙头转向，我还把Attention Is All You Need的论文打印了下来，直接对着英文翻看，效果并不好，反而像是在做英语阅读。

我后来慢慢找到了一个方法：大概就是通过一些通俗易懂的讲解快速入门对一个模型，比如CNN，的一个全局观，知道他的工作原理都有那几步，设置这些步骤分别是为了实现些什么，然后再去找更专业的文章去了解（如上面【注】中的文章），去详细弄懂其中的工作原理和机制，最终再回到英语论文中，和国际接轨，了解原始文献中到底是怎么表述的，论文中是怎么一步步讲解他/她提出的模型的。这样的方法可以让我由通俗理解到深刻理解，最后回到原始论文（毕竟论文本身学术性很高，语言高度精炼浓缩）的时候能更快理解和明白论文的意思。

---
根据我2025/9/27-2025/10/3的学习情况，我下周的目标订立如下：
- [ ] 完成CNN, RNN, LSTM, GRU, Transformer的基础概念学习以及算法推导
- [ ] 读懂Attention Is All You Need
经过上周的摸打滚爬，我有信心在新的一周完成更多知识的学习，请老师批评和指正！